{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Creating Presentations from Notebooks and Python Scrips"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-04T19:12:04.300644Z",
     "start_time": "2023-06-04T19:12:02.733512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import pptx\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.dml.color import RGBColor\n",
    "from pptx.enum.shapes import MSO_SHAPE\n",
    "from pptx.enum.shapes import MSO_SHAPE_TYPE\n",
    "from pptx.enum.shapes import MSO_CONNECTOR\n",
    "from pptx.enum.shapes import MSO_CONNECTOR_TYPE\n",
    "\n",
    "# import placeholders\n",
    "from pptx.enum.shapes import PP_PLACEHOLDER\n",
    "from pptx.enum.shapes import PP_PLACEHOLDER_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pptx.slide.Slide'>  slide_index [ 0 ]\n",
      "index [ 1 ] SUBTITLE (4) Subtitle 3\n",
      "index [ 22 ] PICTURE (18) Picture Placeholder 7\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 1 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 2 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 3 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 4 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 5 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 6 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 7 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 8 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 9 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 10 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 11 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "Number of slides:  12\n"
     ]
    }
   ],
   "source": [
    "path = 'pres/Classification_Challenge.pptx'\n",
    "prs = Presentation(path)\n",
    "\n",
    "for slide in prs.slides:\n",
    "    print(type(slide), slide.name, 'slide_index [',prs.slides.index(slide), ']' )\n",
    "    for p in slide.placeholders:\n",
    "        print('index [', p.placeholder_format.idx, ']', p.placeholder_format.type, p.name)\n",
    "\n",
    "\n",
    "# Get summary details of the presentation\n",
    "number_of_slides = len(prs.slides)\n",
    "print('Number of slides: ', number_of_slides)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T19:12:54.943620Z",
     "start_time": "2023-06-04T19:12:54.896479Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Create lists for slide headers\n",
    "slide_titles = []\n",
    "slide_body = []\n",
    "\n",
    "# Extract slide text\n",
    "slide_text = []\n",
    "for slide in prs.slides:\n",
    "    for shape in slide.shapes:\n",
    "        if not shape.has_text_frame:\n",
    "            continue\n",
    "        for paragraph in shape.text_frame.paragraphs:\n",
    "            for run in paragraph.runs:\n",
    "                slide_text.append(run.text)\n",
    "\n",
    "slide_paragraphs = []\n",
    "for slide in prs.slides:\n",
    "    if slide.shapes.has_text_frame:\n",
    "        slide_paragraphs.append(slide.shapes[3].text)\n",
    "    else:\n",
    "        slide_paragraphs.append('')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T19:14:01.486478Z",
     "start_time": "2023-06-04T19:14:01.449199Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect first slide"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T19:14:02.926740Z",
     "start_time": "2023-06-04T19:14:02.911534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pptx.slide.Slide'>  slide_index [ 0 ]\n",
      "index [ 1 ] SUBTITLE (4) Subtitle 3\n",
      "index [ 22 ] PICTURE (18) Picture Placeholder 7\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 1 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 2 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 3 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 4 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 5 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 6 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 7 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 8 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 9 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 10 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n",
      "<class 'pptx.slide.Slide'>  slide_index [ 11 ]\n",
      "index [ 0 ] TITLE (1) Title 2\n",
      "index [ 17 ] BODY (2) Text Placeholder 1\n",
      "index [ 21 ] BODY (2) Text Placeholder 3\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of slides:  12\n"
     ]
    }
   ],
   "source": [
    "# First slide header\n",
    "print(prs.slides[0].shapes[0].name)\n",
    "print(prs.slides[0].shapes[1].name)\n",
    "print(prs.slides[0].shapes[2].name)\n",
    "print(prs.slides[0].shapes[3].name)\n",
    "print(prs.slides[0].shapes.title.text)\n",
    "print(prs.slides[0].placeholders[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T19:14:20.839815Z",
     "start_time": "2023-06-04T19:14:20.836244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# First shape\n",
    "print(prs.slides[0].shapes.title.text)\n",
    "print(prs.slides[1].shapes[0].name)\n",
    "\n",
    "for shape in slide.shapes:\n",
    "    if not shape.has_text_frame:\n",
    "        continue\n",
    "    text_frame = shape.text_frame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T19:17:30.157721Z",
     "start_time": "2023-06-04T19:17:30.139352Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['Classification Challenge  |   Nils Jennissen',\n '## Task',\n 'Your company, DS Pros, would like to win a contract with a big city council as it would give us great PR. To do so you think it would be a great idea to proactively browse in the open data sets of this city (the one you choose, total freedom here) identify a situation that could be solved or improved using classification algorithms and present it to the technical office of that city council.',\n 'You need to prepare the following:',\n '- A presentation describing the solution you try to solve, how classification will solve it and a summary of the solution proposed',\n '- A well documented and visually appealing notebook where you try different models, explain the steps followed and chose one particular algorithm and hyperparameters (explaining why)',\n '- You should also export that model, once trained, using pickle or similar so it can be reused.',\n '- You should implement a .py script that loads the exported model, accepts a file with samples to classify (identified with an id) and stores the results in a DDBB table (SQLlite) with fields id and class.',\n '- You should provide the files to test the .py script and clear instructions on how to run it.',\n 'Happy coding!!',\n '## 0. Introduction & Proposal',\n 'Thank you very much for the possibility to present our project proposal to you. We are very excited about the opportunity to work with you and hope that you will find our proposal interesting and that we can work together on this project.',\n '**The Problem:**',\n 'The city council of Seattle, Washington, is looking for a solution to predict the weather conditions for the upcoming days based on historical weather data. This would allow them to better plan and prepare for future weather conditions, such as rain, snow, or sunny weather. Especially in a city like Seattle, where the weather is often unpredictable, this would be a very useful tool for the city council to prepare for weather conditions in advance. The current system repeatedly fails to predict the weather correctly, which leads to a lot of frustration and wasted resources.',\n '**The Solution**:',\n \"Our team has developed a cutting-edge weather prediction model that leverages historical weather data and advanced machine learning algorithms to provide highly accurate forecasts for the upcoming days. This solution will enable the city council of Seattle to make well-informed decisions and allocate resources more effectively, ultimately improving the city's preparedness for various weather conditions.\",\n '## 1. Key Features',\n '1. Data-driven approach: Our model utilizes a vast amount of historical weather data, including temperature, humidity, wind speed, and precipitation levels, to identify patterns and trends that can help predict future weather conditions.',\n '2. Advanced machine learning algorithms: We employ state-of-the-art machine learning techniques to analyze the data and generate accurate predictions. These algorithms continuously learn and improve their predictions as more data becomes available.',\n '3. User-friendly visualization: Our solution includes an easy-to-understand visual representation of the predicted weather conditions, using color gradients to indicate the intensity of various weather elements. This allows the city council members to quickly grasp the forecast and make informed decisions.',\n '4. Customizable and scalable: Our weather prediction model can be tailored to the specific needs of the city council and can be easily scaled up to cover larger geographical areas or extended timeframes.',\n '## 2. Benefits for the Council',\n 'By implementing our weather prediction solution, the city council of Seattle will be able to:',\n \"1. Improve preparedness for various weather conditions, reducing the impact of adverse weather on the city's infrastructure and residents.\",\n '2. Optimize resource allocation, ensuring that the necessary resources are available and deployed efficiently during extreme weather events.',\n '3. Enhance communication with the public, providing accurate and timely weather forecasts to help residents plan their activities and stay safe.',\n '4. Save time and money by reducing the reliance on less accurate weather prediction methods and minimizing the consequences of incorrect forecasts.',\n 'We are confident that our weather prediction solution will greatly benefit the city council of Seattle and its residents. We look forward to discussing our proposal further and exploring the possibility of a fruitful collaboration. Thank you for considering our project proposal.',\n '## 3. Setup and tool import',\n 'Throughout this project, various Python libraries are used for data analysis, data visualization, and machine learning. The following libraries are imported for this project:',\n '1. `',\n 'os',\n '`: The `',\n 'os`',\n ' module in Python provides functions for interacting with the operating system, such as file and directory management, environment variables, and process control.',\n '2. `pickle`: The `pickle` module is used for serializing and deserializing Python objects, allowing you to save and load objects to and from files.',\n '3. `pandas`: `pandas` is a powerful data manipulation library that provides data structures like ',\n 'DataFrames',\n ' and Series for handling and analyzing data in a flexible and efficient way.',\n '4. `seaborn`: `seaborn` is a data visualization library based on `matplotlib` that provides a high-level interface for creating informative and attractive statistical graphics.',\n '5. `',\n 'sklearn',\n '`: `scikit-learn` is a popular machine learning library that provides simple and efficient tools for data mining and data analysis, including various classification, regression, and clustering algorithms, as well as tools for model selection and preprocessing.',\n '6. `',\n 'numpy',\n '`: `',\n 'numpy',\n '` is a fundamental library for scientific computing in Python, providing support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.',\n 'We use various classifiers from the `',\n 'sklearn',\n '` library, such as `',\n 'MLPClassifier',\n '`, `',\n 'KNeighborsClassifier',\n '`, `SVC`, `',\n 'GaussianProcessClassifier',\n '`, `',\n 'GradientBoostingClassifier',\n '`, `',\n 'DecisionTreeClassifier',\n '`, `',\n 'ExtraTreesClassifier',\n '`, `',\n 'RandomForestClassifier',\n '`, `',\n 'AdaBoostClassifier',\n '`, `',\n 'GaussianNB',\n '`, `',\n 'QuadraticDiscriminantAnalysis',\n '`, and `',\n 'SGDClassifier',\n '`. Additionally, `',\n 'LabelEncoder',\n '` and `',\n 'StandardScaler',\n '` are used for preprocessing, and `',\n 'GridSearchCV',\n '` is used for hyperparameter tuning.',\n '## 4. The Data',\n 'The Seattle weather dataset is a collection of historical weather data from Seattle, Washington, which can be used for the purpose of weather classification for upcoming days. The dataset  contains information about various weather attributes, such as temperature, precipitation, and weather conditions (e.g., sunny, cloudy, rainy, etc.) for each day.',\n 'The main goal of using this dataset is to train a machine learning model to predict the weather conditions for the upcoming days based on the historical data. By analyzing patterns and trends in the past weather data, the models can learn to recognize the relationships between different weather attributes and make accurate predictions for future weather conditions.',\n 'To achieve this, the dataset is preprocessed and transformed into a suitable format for machine learning algorithms. This may involves handling missing values, converting date-time information into separate features (e.g., day, month, and year), and encoding categorical variables (e.g., weather conditions) into numerical values.',\n '## 5. Data Preprocessing',\n '1. Define a function `',\n 'date_time',\n '(',\n 'df',\n ')` to preprocess the date column:',\n '2. Call the `',\n 'date_time',\n '(',\n 'df',\n ')` function and pass the ',\n 'DataFrame',\n ' to it. This will preprocess the date column and return the modified ',\n 'DataFrame',\n '.',\n '3. Use ',\n 'LabelEncoder',\n \" to convert the text values in the 'weather' column into numeric values (e.g., 1, 2, 3, etc.). Store the encoded values in a new column called '\",\n 'weather_label',\n \"'.\",\n '4. Create a dictionary called `',\n 'weather_dict',\n '` to map the encoded weather labels back to their original text values. Save this dictionary to disk as a pickle file for future use.',\n \"5. Drop the original 'weather' and 'date' columns from the \",\n 'DataFrame',\n ', as they are no longer needed.',\n '6. Split the ',\n 'DataFrame',\n \" into input features (X) and target labels (Y). The input features are all columns except '\",\n 'weather_label',\n \"', and the target labels are the '\",\n 'weather_label',\n \"' column.\",\n '7. Check the shapes of X and Y to ensure they have the correct dimensions.',\n '8. Split the data into training and testing sets using `',\n 'train_test_split',\n '()`. The test set size is 20% of the total data.',\n '9. Scale the input features (',\n 'X_train',\n ' and ',\n 'X_test',\n ') using ',\n 'StandardScaler',\n '. This step is important to ensure that all features have the same scale, which can improve the performance of machine learning algorithms.',\n '## 6. The Models',\n \"Each model uses a different algorithm to learn from the data and make predictions. Here's a brief description of each model:\",\n \"1. **Nearest Neighbors**: This model classifies data points based on their similarity to their nearest neighbors in the training data. It's a simple and intuitive method for classification tasks.\",\n \"2. **Linear SVM**: Linear Support Vector Machine (SVM) is a model that finds the best linear boundary (a straight line in 2D, a plane in 3D, etc.) that separates different classes of data points. It's effective for linearly separable data.\",\n '3. **Polynomial SVM**: This is an extension of the Linear SVM that uses a polynomial function to transform the data into a higher-dimensional space, allowing for more complex decision boundaries.',\n '4. **RBF SVM**: Radial Basis Function (RBF) SVM is another extension of the Linear SVM that uses a non-linear kernel function to transform the data, enabling the model to capture more complex patterns in the data.',\n '5. **Gradient Boosting**: This model combines multiple weak learners (usually decision trees) to create a strong learner. It iteratively improves the model by focusing on the errors made by the previous learners.',\n \"6. **Decision Tree**: This model uses a tree-like structure to make decisions based on the input data. It's easy to understand and interpret, but can be prone to overfitting.\",\n '## 6. The Models',\n \"7. **Extra Trees**: Extra Trees is an ensemble method that builds multiple decision trees and combines their predictions. It's more robust and less prone to overfitting compared to a single decision tree.\",\n '8. **Random Forest**: This model is another ensemble method that builds multiple decision trees and combines their predictions. It introduces randomness in the tree-building process, making it more robust and accurate.',\n \"9. **Neural Net**: This model is inspired by the human brain and consists of interconnected nodes (neurons) that learn to make predictions based on the input data. It's highly flexible and can capture complex patterns in the data.\",\n '10. **AdaBoost**: AdaBoost is an ensemble method that combines multiple weak learners (usually decision trees) to create a strong learner. It focuses on the errors made by the previous learners and assigns more weight to the misclassified data points.',\n \"11. **Naive Bayes**: This model is based on the Bayes' theorem and assumes that the features in the data are independent. It's simple, fast, and works well with small datasets.\",\n \"12. **SGD**: Stochastic Gradient Descent (SGD) is an optimization algorithm used to train various types of models, including linear classifiers. It's efficient and works well with large datasets.\",\n 'These models can be used individually or combined to create a more accurate and robust prediction system.',\n '## 7. Evaluation',\n 'The results point towards using the Random Forest Classifier as it has the highest accuracy score of 0.86.',\n 'For the Random Forest Classifier the best results were yielded for the bright yellow visualized parameters.',\n 'We will use a ',\n 'modell',\n ' with 40 estimators and 3 max features, due to the peak in the accuracy score.',\n '## ',\n '8',\n '. Usage of the .',\n 'py',\n ' file for predictions',\n 'To predict the weather in Seattle for tomorrow using the given .py file, follow these steps:',\n \"1. Update the data file: Replace the `seattle-weather.csv` file in the `./FinalProject/data/` directory with a new file containing the weather data for Seattle, including tomorrow's date. Make sure the new file has the same format and columns as the original file.\",\n '2. Run the script: Execute the .py script. This will perform the following actions:',\n '   - Import necessary libraries.',\n '   - Load the pre-trained model from the `best_model.sav` file.',\n '   - Read the updated weather data from the `seattle-weather.csv` file.',\n '   - Preprocess the data by transforming the date and scaling the numerical values.',\n '   - Predict the weather for each entry in the data using the loaded model.',\n \"   - Convert the predicted label numbers to label names (e.g., 0 to 'drizzle', 1 to 'rain', etc.).\",\n '   - Create or connect to an SQLite database named `classification.db`.',\n '   - Create a table named `classification` with fields `id`, `class`, and `class_name`.',\n '   - Insert the predicted weather data into the `classification` table.',\n '   - Save the changes to the database and close the connection.',\n '3. Check the prediction: Open the `classification.db` file using an SQLite database viewer, and look for the row with the date corresponding to tomorrow. The `class_name` field in that row will contain the predicted weather for Seattle tomorrow.']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with the shape information of each slide in a column\n",
    "prs = Presentation()\n",
    "for slide in prs.slides:\n",
    "    print(slide.title)\n",
    "    for shape in slide.placeholders:\n",
    "        print('%d %s' % (shape.placeholder_format.idx, shape.name))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T19:17:44.051889Z",
     "start_time": "2023-06-04T19:17:44.045952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['',\n 'Classification Challenge  |   Nils Jennissen',\n '',\n '',\n '',\n '## Task',\n 'Your company, DS Pros, would like to win a contract with a big city council as it would give us great PR. To do so you think it would be a great idea to proactively browse in the open data sets of this city (the one you choose, total freedom here) identify a situation that could be solved or improved using classification algorithms and present it to the technical office of that city council.\\n\\nYou need to prepare the following:\\n\\n- A presentation describing the solution you try to solve, how classification will solve it and a summary of the solution proposed\\n- A well documented and visually appealing notebook where you try different models, explain the steps followed and chose one particular algorithm and hyperparameters (explaining why)\\n- You should also export that model, once trained, using pickle or similar so it can be reused.\\n- You should implement a .py script that loads the exported model, accepts a file with samples to classify (identified with an id) and stores the results in a DDBB table (SQLlite) with fields id and class.\\n- You should provide the files to test the .py script and clear instructions on how to run it.\\n\\n\\nHappy coding!!\\n',\n '',\n '## 0. Introduction & Proposal',\n \"\\nThank you very much for the possibility to present our project proposal to you. We are very excited about the opportunity to work with you and hope that you will find our proposal interesting and that we can work together on this project.\\n\\n**The Problem:**\\nThe city council of Seattle, Washington, is looking for a solution to predict the weather conditions for the upcoming days based on historical weather data. This would allow them to better plan and prepare for future weather conditions, such as rain, snow, or sunny weather. Especially in a city like Seattle, where the weather is often unpredictable, this would be a very useful tool for the city council to prepare for weather conditions in advance. The current system repeatedly fails to predict the weather correctly, which leads to a lot of frustration and wasted resources.\\n\\n**The Solution**:\\nOur team has developed a cutting-edge weather prediction model that leverages historical weather data and advanced machine learning algorithms to provide highly accurate forecasts for the upcoming days. This solution will enable the city council of Seattle to make well-informed decisions and allocate resources more effectively, ultimately improving the city's preparedness for various weather conditions.\\n\",\n '',\n '## 1. Key Features',\n '\\n1. Data-driven approach: Our model utilizes a vast amount of historical weather data, including temperature, humidity, wind speed, and precipitation levels, to identify patterns and trends that can help predict future weather conditions.\\n\\n2. Advanced machine learning algorithms: We employ state-of-the-art machine learning techniques to analyze the data and generate accurate predictions. These algorithms continuously learn and improve their predictions as more data becomes available.\\n\\n3. User-friendly visualization: Our solution includes an easy-to-understand visual representation of the predicted weather conditions, using color gradients to indicate the intensity of various weather elements. This allows the city council members to quickly grasp the forecast and make informed decisions.\\n\\n4. Customizable and scalable: Our weather prediction model can be tailored to the specific needs of the city council and can be easily scaled up to cover larger geographical areas or extended timeframes.\\n',\n '',\n '## 2. Benefits for the Council',\n \"\\nBy implementing our weather prediction solution, the city council of Seattle will be able to:\\n\\n1. Improve preparedness for various weather conditions, reducing the impact of adverse weather on the city's infrastructure and residents.\\n\\n2. Optimize resource allocation, ensuring that the necessary resources are available and deployed efficiently during extreme weather events.\\n\\n3. Enhance communication with the public, providing accurate and timely weather forecasts to help residents plan their activities and stay safe.\\n\\n4. Save time and money by reducing the reliance on less accurate weather prediction methods and minimizing the consequences of incorrect forecasts.\\n\\nWe are confident that our weather prediction solution will greatly benefit the city council of Seattle and its residents. We look forward to discussing our proposal further and exploring the possibility of a fruitful collaboration. Thank you for considering our project proposal.\\n\",\n '',\n '## 3. Setup and tool import',\n 'Throughout this project, various Python libraries are used for data analysis, data visualization, and machine learning. The following libraries are imported for this project:\\n\\n1. `os`: The `os` module in Python provides functions for interacting with the operating system, such as file and directory management, environment variables, and process control.\\n\\n2. `pickle`: The `pickle` module is used for serializing and deserializing Python objects, allowing you to save and load objects to and from files.\\n\\n3. `pandas`: `pandas` is a powerful data manipulation library that provides data structures like DataFrames and Series for handling and analyzing data in a flexible and efficient way.\\n\\n4. `seaborn`: `seaborn` is a data visualization library based on `matplotlib` that provides a high-level interface for creating informative and attractive statistical graphics.\\n\\n5. `sklearn`: `scikit-learn` is a popular machine learning library that provides simple and efficient tools for data mining and data analysis, including various classification, regression, and clustering algorithms, as well as tools for model selection and preprocessing.\\n\\n6. `numpy`: `numpy` is a fundamental library for scientific computing in Python, providing support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\\n\\nWe use various classifiers from the `sklearn` library, such as `MLPClassifier`, `KNeighborsClassifier`, `SVC`, `GaussianProcessClassifier`, `GradientBoostingClassifier`, `DecisionTreeClassifier`, `ExtraTreesClassifier`, `RandomForestClassifier`, `AdaBoostClassifier`, `GaussianNB`, `QuadraticDiscriminantAnalysis`, and `SGDClassifier`. Additionally, `LabelEncoder` and `StandardScaler` are used for preprocessing, and `GridSearchCV` is used for hyperparameter tuning.\\n\\n',\n '',\n '## 4. The Data',\n 'The Seattle weather dataset is a collection of historical weather data from Seattle, Washington, which can be used for the purpose of weather classification for upcoming days. The dataset  contains information about various weather attributes, such as temperature, precipitation, and weather conditions (e.g., sunny, cloudy, rainy, etc.) for each day.\\n\\nThe main goal of using this dataset is to train a machine learning model to predict the weather conditions for the upcoming days based on the historical data. By analyzing patterns and trends in the past weather data, the models can learn to recognize the relationships between different weather attributes and make accurate predictions for future weather conditions.\\n\\nTo achieve this, the dataset is preprocessed and transformed into a suitable format for machine learning algorithms. This may involves handling missing values, converting date-time information into separate features (e.g., day, month, and year), and encoding categorical variables (e.g., weather conditions) into numerical values.\\n',\n '',\n '## 5. Data Preprocessing',\n \"\\n1. Define a function `date_time(df)` to preprocess the date column:\\n\\n2. Call the `date_time(df)` function and pass the DataFrame to it. This will preprocess the date column and return the modified DataFrame.\\n\\n3. Use LabelEncoder to convert the text values in the 'weather' column into numeric values (e.g., 1, 2, 3, etc.). Store the encoded values in a new column called 'weather_label'.\\n\\n4. Create a dictionary called `weather_dict` to map the encoded weather labels back to their original text values. Save this dictionary to disk as a pickle file for future use.\\n\\n5. Drop the original 'weather' and 'date' columns from the DataFrame, as they are no longer needed.\\n\\n6. Split the DataFrame into input features (X) and target labels (Y). The input features are all columns except 'weather_label', and the target labels are the 'weather_label' column.\\n\\n7. Check the shapes of X and Y to ensure they have the correct dimensions.\\n\\n8. Split the data into training and testing sets using `train_test_split()`. The test set size is 20% of the total data.\\n\\n9. Scale the input features (X_train and X_test) using StandardScaler. This step is important to ensure that all features have the same scale, which can improve the performance of machine learning algorithms.\",\n '',\n '## 6. The Models',\n \"Each model uses a different algorithm to learn from the data and make predictions. Here's a brief description of each model:\\n\\n1. **Nearest Neighbors**: This model classifies data points based on their similarity to their nearest neighbors in the training data. It's a simple and intuitive method for classification tasks.\\n\\n2. **Linear SVM**: Linear Support Vector Machine (SVM) is a model that finds the best linear boundary (a straight line in 2D, a plane in 3D, etc.) that separates different classes of data points. It's effective for linearly separable data.\\n\\n3. **Polynomial SVM**: This is an extension of the Linear SVM that uses a polynomial function to transform the data into a higher-dimensional space, allowing for more complex decision boundaries.\\n\\n4. **RBF SVM**: Radial Basis Function (RBF) SVM is another extension of the Linear SVM that uses a non-linear kernel function to transform the data, enabling the model to capture more complex patterns in the data.\\n\\n5. **Gradient Boosting**: This model combines multiple weak learners (usually decision trees) to create a strong learner. It iteratively improves the model by focusing on the errors made by the previous learners.\\n\\n6. **Decision Tree**: This model uses a tree-like structure to make decisions based on the input data. It's easy to understand and interpret, but can be prone to overfitting.\\n\",\n '',\n '## 6. The Models',\n \"\\n7. **Extra Trees**: Extra Trees is an ensemble method that builds multiple decision trees and combines their predictions. It's more robust and less prone to overfitting compared to a single decision tree.\\n\\n8. **Random Forest**: This model is another ensemble method that builds multiple decision trees and combines their predictions. It introduces randomness in the tree-building process, making it more robust and accurate.\\n\\n9. **Neural Net**: This model is inspired by the human brain and consists of interconnected nodes (neurons) that learn to make predictions based on the input data. It's highly flexible and can capture complex patterns in the data.\\n\\n10. **AdaBoost**: AdaBoost is an ensemble method that combines multiple weak learners (usually decision trees) to create a strong learner. It focuses on the errors made by the previous learners and assigns more weight to the misclassified data points.\\n\\n11. **Naive Bayes**: This model is based on the Bayes' theorem and assumes that the features in the data are independent. It's simple, fast, and works well with small datasets.\\n\\n12. **SGD**: Stochastic Gradient Descent (SGD) is an optimization algorithm used to train various types of models, including linear classifiers. It's efficient and works well with large datasets.\\n\\nThese models can be used individually or combined to create a more accurate and robust prediction system.\\n\",\n '',\n '## 7. Evaluation',\n 'The results point towards using the Random Forest Classifier as it has the highest accuracy score of 0.86.\\n\\nFor the Random Forest Classifier the best results were yielded for the bright yellow visualized parameters.\\nWe will use a modell with 40 estimators and 3 max features, due to the peak in the accuracy score.\\n\\n',\n '',\n '',\n '',\n '',\n '## 8. Usage of the .py file for predictions',\n \"To predict the weather in Seattle for tomorrow using the given .py file, follow these steps:\\n\\n1. Update the data file: Replace the `seattle-weather.csv` file in the `./FinalProject/data/` directory with a new file containing the weather data for Seattle, including tomorrow's date. Make sure the new file has the same format and columns as the original file.\\n\\n2. Run the script: Execute the .py script. This will perform the following actions:\\n   - Import necessary libraries.\\n   - Load the pre-trained model from the `best_model.sav` file.\\n   - Read the updated weather data from the `seattle-weather.csv` file.\\n   - Preprocess the data by transforming the date and scaling the numerical values.\\n   - Predict the weather for each entry in the data using the loaded model.\\n   - Convert the predicted label numbers to label names (e.g., 0 to 'drizzle', 1 to 'rain', etc.).\\n   - Create or connect to an SQLite database named `classification.db`.\\n   - Create a table named `classification` with fields `id`, `class`, and `class_name`.\\n   - Insert the predicted weather data into the `classification` table.\\n   - Save the changes to the database and close the connection.\\n\\n3. Check the prediction: Open the `classification.db` file using an SQLite database viewer, and look for the row with the date corresponding to tomorrow. The `class_name` field in that row will contain the predicted weather for Seattle tomorrow.\\n\"]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prs = Presentation()\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[3])\n",
    "for shape in slide.shapes:\n",
    "    print('%s' % shape.shape_type)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T19:18:51.683288Z",
     "start_time": "2023-06-04T19:18:51.679322Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "slide = prs.slides.add_slide(prs.slides[3].shapes)\n",
    "for shape in slide.shapes:\n",
    "    print(shape.name, shape.shape_type, shape.has_text_frame, shape.is_placeholder)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
